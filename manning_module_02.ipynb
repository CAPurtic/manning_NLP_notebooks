{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd057990f1ad2ea89c67ddae7f31d40c478205c5912da0fccfb7c5cfbb2b8bf17ad",
   "display_name": "Python 3.9.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import matplotlib\n",
    "import nltk\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.sentiment.util\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.0\n",
    "Load in the small_corpus.csv file you created in the previous milestone.\n",
    "\"\"\"\n",
    "\n",
    "small_corpus = pd.read_csv(\"small_corpus.csv\")\n",
    "small_corpus = small_corpus.drop(\"Unnamed: 0\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.0\n",
    "Tokenize the sentences and words of the reviews with the tokenize module of NLTK.\n",
    "\n",
    "Keep in mind that word_tokenize and sent_tokenize functions of the nltk.tokenize module should be used.\n",
    "\"\"\"\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    \"\"\"Splits paragraphs to sentences, then sentences to bags of words.\n",
    "\n",
    "        Function breaks longer reviews into sentences and then breaks,\n",
    "        the sentences into individual words.\n",
    "        \n",
    "        Args:\n",
    "            sentence: a single, or multiple, sentence as a str.\n",
    "        \n",
    "        Returns:\n",
    "            List of lists of each sentence, broken into words.\n",
    "        \"\"\"\n",
    "    tokenized_sentence = sent_tokenize(sentence)\n",
    "    return_list = []\n",
    "    for s in tokenized_sentence:\n",
    "        words = word_tokenize(s)\n",
    "        words = [word.lower() for word in words if word.isalnum()]\n",
    "        if words != []:\n",
    "            return_list.append(words)\n",
    "        elif words == []:\n",
    "            continue\n",
    "    return return_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package opinion_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.0\n",
    "Download the opinion lexicon of NLTK by using the following command: nltk.download('opinion_lexicon'). Before you classify each word of the reviews, experiment with words and find out whether they are labeled as positive or negative.\n",
    "\n",
    "Note that the dictionary contains various word-forms, not only stems.\n",
    "\"\"\"\n",
    "\n",
    "nltk.download(\"opinion_lexicon\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentalyzer(review):\n",
    "    \"\"\"Calculates a numeric representation of sentence sentiment (-1 to +1).\n",
    "\n",
    "        Function takes in tokenized reviews, calculates a sentiment for\n",
    "        each sentence as a positive or negative percentage of the total\n",
    "        words in a sentence. Then calculates a sentiment for the entire\n",
    "        review as a sum of the sentence sentiments, capped at either -1\n",
    "        or +1.\n",
    "        \n",
    "        Args:\n",
    "            review: a tokenized sentence(s), as a list of lists\n",
    "        \n",
    "        Returns:\n",
    "            A numeric sentiment, capped from -1 to +1.\n",
    "        \"\"\"\n",
    "\n",
    "    tokenized = tokenizer(review)\n",
    "    review_polarity = 0\n",
    "    for sentence in tokenized:\n",
    "        positive_word_count = 0\n",
    "        negative_word_count = 0\n",
    "        neutral_word_count = 0\n",
    "        sentence_polarity = 0\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in opinion_lexicon.positive():\n",
    "                positive_word_count += 1\n",
    "            elif word in opinion_lexicon.negative():\n",
    "                negative_word_count += 1\n",
    "            else:\n",
    "                neutral_word_count += 1\n",
    "        \n",
    "        total_word_count = positive_word_count + negative_word_count + neutral_word_count\n",
    "        if positive_word_count > negative_word_count:\n",
    "            sentence_polarity = positive_word_count / total_word_count\n",
    "        elif negative_word_count > positive_word_count:\n",
    "            sentence_polarity = -(negative_word_count / total_word_count)\n",
    "        else:\n",
    "            sentence_polarity = 0\n",
    "        review_polarity += sentence_polarity\n",
    "\n",
    "    if review_polarity > 1:\n",
    "        review_polarity = 1\n",
    "    elif review_polarity < -1:\n",
    "        review_polarity = -1\n",
    "    return review_polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentalyzer_mark_neg(review):\n",
    "    \"\"\"Calculates a numeric representation of sentence sentiment (-1 to +1).\n",
    "\n",
    "        Function takes in tokenized reviews, calculates a sentiment for\n",
    "        each sentence as a positive or negative percentage of the total\n",
    "        words in a sentence. Then calculates a sentiment for the entire\n",
    "        review as a sum of the sentence sentiments, capped at either -1\n",
    "        or +1.\n",
    "\n",
    "        **Function is based on sentimentalyzer() but includes additional\n",
    "        code to catch negation words.\n",
    "        \n",
    "        Args:\n",
    "            review: a tokenized sentence(s), as a list of lists\n",
    "        \n",
    "        Returns:\n",
    "            A numeric sentiment, capped from -1 to +1.\n",
    "        \"\"\"\n",
    "\n",
    "    tokenized = tokenizer(review)\n",
    "    review_polarity = 0\n",
    "    for sentence in tokenized:\n",
    "        sentence = nltk.sentiment.util.mark_negation(sentence)\n",
    "        positive_word_count = 0\n",
    "        negative_word_count = 0\n",
    "        neutral_word_count = 0\n",
    "        sentence_polarity = 0\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in opinion_lexicon.positive():\n",
    "                positive_word_count += 1\n",
    "            elif word in opinion_lexicon.negative() or word.find(\"_NEG\") != -1:\n",
    "                negative_word_count += 1\n",
    "            else:\n",
    "                neutral_word_count += 1\n",
    "        \n",
    "        total_word_count = positive_word_count + negative_word_count + neutral_word_count\n",
    "        if positive_word_count > negative_word_count:\n",
    "            sentence_polarity = positive_word_count / total_word_count\n",
    "        elif negative_word_count > positive_word_count:\n",
    "            sentence_polarity = -(negative_word_count / total_word_count)\n",
    "        else:\n",
    "            sentence_polarity = 0\n",
    "        review_polarity += sentence_polarity\n",
    "\n",
    "    if review_polarity > 1:\n",
    "        review_polarity = 1\n",
    "    elif review_polarity < -1:\n",
    "        review_polarity = -1\n",
    "    return review_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentalyzer_mark_neg_stop_words(review):\n",
    "    \"\"\"Calculates a numeric representation of sentence sentiment (-1 to +1).\n",
    "\n",
    "        Function takes in tokenized reviews, calculates a sentiment for\n",
    "        each sentence as a positive or negative percentage of the total\n",
    "        words in a sentence. Then calculates a sentiment for the entire\n",
    "        review as a sum of the sentence sentiments, capped at either -1\n",
    "        or +1.\n",
    "\n",
    "        **Function is based on sentimentalyzer() but includes additional\n",
    "        code to catch negation words and stop words.\n",
    "        \n",
    "        Args:\n",
    "            review: a tokenized sentence(s), as a list of lists\n",
    "        \n",
    "        Returns:\n",
    "            A numeric sentiment, capped from -1 to +1.\n",
    "        \"\"\"\n",
    "\n",
    "    tokenized = tokenizer(review)\n",
    "    review_polarity = 0\n",
    "    for sentence in tokenized:\n",
    "        sentence = [word for word in sentence if not word in stopwords.words(\"english\")]\n",
    "        sentence = nltk.sentiment.util.mark_negation(sentence)\n",
    "        positive_word_count = 0\n",
    "        negative_word_count = 0\n",
    "        neutral_word_count = 0\n",
    "        sentence_polarity = 0\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in opinion_lexicon.positive():\n",
    "                positive_word_count += 1\n",
    "            elif word in opinion_lexicon.negative() or word.find(\"_NEG\") != -1:\n",
    "                negative_word_count += 1\n",
    "            else:\n",
    "                neutral_word_count += 1\n",
    "        \n",
    "        total_word_count = positive_word_count + negative_word_count + neutral_word_count\n",
    "        if positive_word_count > negative_word_count:\n",
    "            sentence_polarity = positive_word_count / total_word_count\n",
    "        elif negative_word_count > positive_word_count:\n",
    "            sentence_polarity = -(negative_word_count / total_word_count)\n",
    "        else:\n",
    "            sentence_polarity = 0\n",
    "        review_polarity += sentence_polarity\n",
    "\n",
    "    if review_polarity > 1:\n",
    "        review_polarity = 1\n",
    "    elif review_polarity < -1:\n",
    "        review_polarity = -1\n",
    "    return review_polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentiment_score(dataframe, new_column, target_column, function_to_apply):\n",
    "    \"\"\"Applies a custom function to a target column and adds results as a new column.\n",
    "\n",
    "        Function takes in a dataframe and applies a user defined function\n",
    "        against a user defined column and saves the results in a new column.\n",
    "        \n",
    "        Args:\n",
    "            dataframe: any dataframe\n",
    "            new_column: the name of the column that will hold the function results\n",
    "            target_column: the column in the dataframe to apply the function to\n",
    "            function_to_apply: the function to be applied to the target column\n",
    "        \n",
    "        Returns:\n",
    "            The original dataframe with a new column.\n",
    "        \"\"\"\n",
    "    \n",
    "    dataframe[new_column] = dataframe[target_column].map(lambda s: function_to_apply(s))\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(dataframe, file_name, cols, header_titles):\n",
    "    \"\"\"Saves the passed dataframe to a .csv\"\"\"\n",
    "    dataframe.to_csv(file_name, columns=cols, header=header_titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_histogram(dataframe, x, y, file_name):\n",
    "    \"\"\"Creates a histogram and saves to file.\"\"\"\n",
    "    chart = (\n",
    "    alt.Chart(dataframe)\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        )\n",
    "    )\n",
    "    alt.renderers.enable(\"altair_viewer\")\n",
    "    chart.save(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scatter_plot(dataframe, x, y, file_name):\n",
    "    \"\"\"Creates a scatter plot and saves to file.\"\"\"\n",
    "    chart = (\n",
    "    alt.Chart(dataframe)\n",
    "    .mark_circle(size=60)\n",
    "    .encode(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        )\n",
    "    )\n",
    "    alt.renderers.enable(\"altair_viewer\")\n",
    "    chart.save(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scipy_correlation(x, y, method=\"pearson\"):\n",
    "    \"\"\"Print the correlation between two pandas series.\"\"\"\n",
    "    try:\n",
    "        if method==\"pearson\":\n",
    "            r, p = scipy.stats.pearsonr(x, y)\n",
    "        elif method==\"spearman\":\n",
    "            r, p = scipy.stats.spearmanr(x, y)\n",
    "        elif method==\"kendalltau\":\n",
    "            r, p = scipy.stats.kendalltau(x, y)\n",
    "        print(f\"Using {method} the R-value is: {r}, and it has a p-value of: {p}\")\n",
    "    except:\n",
    "        print(\"That is not one of the available correlation methods.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scipy_linreg(x, y):\n",
    "    \"\"\"Print the intercept and coefficient of a linear regression.\"\"\"\n",
    "    regr = scipy.stats.linregress(x, y)\n",
    "    print(f\"Intercept value is: {regr.intercept}.\")\n",
    "    print(f\"Coefficient value is: {regr.slope}.\")\n",
    "    print(f\"Coefficient value is: {regr.slope}.\")\n",
    "    print(f\"p-value is: {regr.pvalue}.\")\n",
    "    print(f\"R-value is: {regr.rvalue}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_sentiment_scores(dataframe, column, breaks=[-1, -.75, -0.5, -0.25, 0, 0.25, 0.50, 0.75, 1, 1.25]):\n",
    "    \"\"\"Returns histogram values as a dataframe.\"\"\"\n",
    "    count, division = np.histogram(dataframe[column], breaks)\n",
    "    sentiment_histogram = pd.DataFrame({\"count\": count, \"division\": division[:-1]})\n",
    "    return sentiment_histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      ratings                                  reviews  sentiment_score  \\\n",
       "0           1                  THE DAY GAMING CRIED...         0.000000   \n",
       "1           1                                 One Star         0.000000   \n",
       "2           1  these do not work at all, all i get ...         0.111111   \n",
       "3           1                            last gen game         0.000000   \n",
       "4           1                                    Waste        -1.000000   \n",
       "...       ...                                      ...              ...   \n",
       "4495        5                               Five Stars         0.000000   \n",
       "4496        5                               Five Stars         0.000000   \n",
       "4497        5                               Five Stars         0.000000   \n",
       "4498        5                                 Awesome!         1.000000   \n",
       "4499        5                               great time         0.500000   \n",
       "\n",
       "      sentiment_score_mark_neg  sentiment_score_mark_neg_stop_words  \n",
       "0                     0.000000                                  0.0  \n",
       "1                     0.000000                                  0.0  \n",
       "2                    -0.666667                                  0.5  \n",
       "3                     0.000000                                  0.0  \n",
       "4                    -1.000000                                 -1.0  \n",
       "...                        ...                                  ...  \n",
       "4495                  0.000000                                  0.0  \n",
       "4496                  0.000000                                  0.0  \n",
       "4497                  0.000000                                  0.0  \n",
       "4498                  1.000000                                  1.0  \n",
       "4499                  0.500000                                  0.5  \n",
       "\n",
       "[4500 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ratings</th>\n      <th>reviews</th>\n      <th>sentiment_score</th>\n      <th>sentiment_score_mark_neg</th>\n      <th>sentiment_score_mark_neg_stop_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>THE DAY GAMING CRIED...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>One Star</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>these do not work at all, all i get ...</td>\n      <td>0.111111</td>\n      <td>-0.666667</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>last gen game</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>Waste</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4495</th>\n      <td>5</td>\n      <td>Five Stars</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4496</th>\n      <td>5</td>\n      <td>Five Stars</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4497</th>\n      <td>5</td>\n      <td>Five Stars</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4498</th>\n      <td>5</td>\n      <td>Awesome!</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4499</th>\n      <td>5</td>\n      <td>great time</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.5</td>\n    </tr>\n  </tbody>\n</table>\n<p>4500 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.0\n",
    "Classify each review in a scale of –1 to +1. The higher the score is, the more positive the review is.\n",
    "\n",
    "It is recommended to score the reviews in two steps. First score the sentences of the reviews from –1 to 1 based on the sum of the positive and negative words they include. Then count the sentiment score of the reviews, which you preliminary sliced into sentences.\n",
    "Don’t forget that NLTK opinion lexicon neither contains uppercase words, nor punctuation marks.\n",
    "\"\"\"\n",
    "\n",
    "generate_sentiment_score(small_corpus, \"sentiment_score\", \"reviews\", function_to_apply=sentimentalyzer)\n",
    "generate_sentiment_score(small_corpus, \"sentiment_score_mark_neg\", \"reviews\", function_to_apply=sentimentalyzer_mark_neg)\n",
    "generate_sentiment_score(small_corpus, \"sentiment_score_mark_neg_stop_words\", \"reviews\", function_to_apply=sentimentalyzer_mark_neg_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(small_corpus, \"small_corpus_sentiment.csv\", cols=[\"ratings\", \"reviews\", \"sentiment_score\", \"sentiment_score_mark_neg\", \"sentiment_score_mark_neg_stop_words\"], header_titles=[\"ratings\", \"reviews\", \"sentiment_score\", \"sentiment_score_mark_neg\", \"sentiment_score_mark_neg_stop_words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.0\n",
    "Compare the scores of the product reviews with the product ratings using a plot. In this step, you need to accomplish three sub-tasks.\n",
    "\n",
    "5.1 \n",
    "Create a plot of the distribution of the ratings. Explore which is the most common rating.\n",
    "\n",
    "You can use Altair to create the plot.\n",
    "\"\"\"\n",
    "\n",
    "create_histogram(small_corpus, \"ratings\", \"count()\", \"small_corpus_ratings_histogram.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.2 \n",
    "Create a plot of the distribution of the sentiment scores. Explore which is the most common.\n",
    "\n",
    "Note that the scores are not discrete numbers.\n",
    "It is recommended to use NumPy histogram to put the sentiment scores into bins.\n",
    "\"\"\"\n",
    "\n",
    "sentiment_histogram = bin_sentiment_scores(small_corpus, \"sentiment_score\")\n",
    "create_histogram(sentiment_histogram, \"division\", \"count\", \"small_corpus_sentiment_histogram.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.3 \n",
    "Create a plot about the relation of the sentiment scores and product ratings. What is your impression? Do they correlate?\n",
    "\"\"\"\n",
    "\n",
    "create_scatter_plot(small_corpus, \"ratings\", \"sentiment_score\", \"ratings_vs_sentiment_scatter.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using pearson the R-value is: 0.4203153729322831, and it has a p-value of: 3.829044443872697e-192\n\n\nUsing spearman the R-value is: 0.431086557814956, and it has a p-value of: 4.2847721094879914e-203\n\n\nUsing kendalltau the R-value is: 0.34825337832628694, and it has a p-value of: 7.62443898302215e-187\n\n\nIntercept value is: -0.2071126957347555.\nCoefficient value is: 0.0803880898755916.\nCoefficient value is: 0.0803880898755916.\np-value is: 3.829044443866182e-192.\nR-value is: 0.4203153729322852.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "6.0 \n",
    "Measure the correlation of the sentiment scores and product ratings. Try out more methods. Study the contradictions, namely those cases where the rating is high but the score is low, or the other way around.\n",
    "\n",
    "Choose the most effective correlation measure.\n",
    "\"\"\"\n",
    "corr_methods = [\"pearson\", \"spearman\", \"kendalltau\"]\n",
    "for m in corr_methods:\n",
    "    scipy_correlation(small_corpus[\"ratings\"], small_corpus[\"sentiment_score\"], method=m)\n",
    "    print(\"\\n\")\n",
    "scipy_linreg(small_corpus[\"ratings\"], small_corpus[\"sentiment_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7.0\n",
    "Improve your sentiment analyzer in order to reduce contradictory cases. Handle negation, as most of those cases are contradictory when there is negation in the sentence (e.g., no problem).\n",
    "\n",
    "It is recommended to use the mark_negation function of the NLTK sentiment.utils module.\n",
    "Don’t forget to complete your vocabulary with negated words.\n",
    "\"\"\"\n",
    "\n",
    "sentiment_histogram = bin_sentiment_scores(small_corpus, \"sentiment_score_mark_neg\")\n",
    "create_histogram(sentiment_histogram, \"division\", \"count\", \"small_corpus_sentiment_mark_neg_histogram.html\")\n",
    "\n",
    "\n",
    "sentiment_histogram = bin_sentiment_scores(small_corpus, \"sentiment_score_mark_neg_stop_words\")\n",
    "create_histogram(sentiment_histogram, \"division\", \"count\", \"small_corpus_sentiment_mark_neg_stop_words_histogram.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_scatter_plot(small_corpus, \"ratings\", \"sentiment_score_mark_neg\", \"ratings_vs_sentiment_mark_neg_scatter.html\")\n",
    "create_scatter_plot(small_corpus, \"ratings\", \"sentiment_score_mark_neg_stop_words\", \"ratings_vs_sentiment_mark_neg_stop_words_scatter.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using pearson the R-value is: 0.4627921104296059, and it has a p-value of: 9.89857705849804e-238\n\n\nUsing spearman the R-value is: 0.4907435118544055, and it has a p-value of: 1.852594628034942e-271\n\n\nUsing kendalltau the R-value is: 0.3913151273993718, and it has a p-value of: 3.81903593259519e-239\n\n\nIntercept value is: -0.34139602551167797.\nCoefficient value is: 0.10194331057012819.\nCoefficient value is: 0.10194331057012819.\np-value is: 9.89857705841941e-238.\nR-value is: 0.46279211042961005.\n\n\nUsing pearson the R-value is: 0.3897947656165339, and it has a p-value of: 3.276856828537041e-163\n\n\nUsing spearman the R-value is: 0.4158543064820247, and it has a p-value of: 1.0066814665397989e-187\n\n\nUsing kendalltau the R-value is: 0.33656206645715886, and it has a p-value of: 1.1511098722279842e-172\n\n\nIntercept value is: -0.23923025298538136.\nCoefficient value is: 0.09705425045040435.\nCoefficient value is: 0.09705425045040435.\np-value is: 3.2768568285364585e-163.\nR-value is: 0.3897947656165354.\n"
     ]
    }
   ],
   "source": [
    "for m in corr_methods:\n",
    "    scipy_correlation(small_corpus[\"ratings\"], small_corpus[\"sentiment_score_mark_neg\"], method=m)\n",
    "    print(\"\\n\")\n",
    "scipy_linreg(small_corpus[\"ratings\"], small_corpus[\"sentiment_score_mark_neg\"])\n",
    "\n",
    "print(\"\\n\")\n",
    "for m in corr_methods:\n",
    "    scipy_correlation(small_corpus[\"ratings\"], small_corpus[\"sentiment_score_mark_neg_stop_words\"], method=m)\n",
    "    print(\"\\n\")\n",
    "scipy_linreg(small_corpus[\"ratings\"], small_corpus[\"sentiment_score_mark_neg_stop_words\"])\n"
   ]
  }
 ]
}